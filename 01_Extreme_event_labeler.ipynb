{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed63bb4-1aab-4fc4-91aa-3a8aaf225a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bb592-a66e-48fe-a851-98d13e261572",
   "metadata": {},
   "source": [
    "## Step 1: Load and clean tower data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ebed6b-aa61-43ac-b538-60726777198b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jose/DATA_WEATHER_ORNL/data/met_towers_2017-2022_final-qc/TOWA_2017-2022_final-qc.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m tower_dfs_15m_clean = []\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tower \u001b[38;5;129;01min\u001b[39;00m towers_of_interest:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mGLOBAL_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/met_towers_2017-2022_final-qc/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_2017-2022_final-qc.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-999\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m         df[\u001b[33m'\u001b[39m\u001b[33mtimestampUTC\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mtimestampUTC\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m).dt.tz_localize(\u001b[33m'\u001b[39m\u001b[33mUTC\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     23\u001b[39m         df = df.set_index(\u001b[33m'\u001b[39m\u001b[33mtimestampUTC\u001b[39m\u001b[33m'\u001b[39m, drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ml_env2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ml_env2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ml_env2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ml_env2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ml_env2/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/jose/DATA_WEATHER_ORNL/data/met_towers_2017-2022_final-qc/TOWA_2017-2022_final-qc.csv'"
     ]
    }
   ],
   "source": [
    "GLOBAL_PATH=\"/home/jose/DATA_WEATHER_ORNL/data\"\n",
    "\n",
    "# Specify towers and variables of interest\n",
    "towers_of_interest = ['TOWA', 'TOWB', 'TOWD', 'TOWF', 'TOWS', 'TOWY']\n",
    "vars = ['TempC',\n",
    "        'RelHum', 'AbsHum', \n",
    "        'WSpdMph', 'PkWSpdMph', 'VSSpdMph',\n",
    "        'SolarRadWm2', \n",
    "        'BarPresMb',\n",
    "        'Sigma', 'SigPhi',\n",
    "        'WDir',\n",
    "        'PrecipIn']\n",
    "\n",
    "# Load final quality assessed data and format datetime\n",
    "tower_dfs_15m_clean = []\n",
    "for tower in towers_of_interest:\n",
    "\n",
    "        df = pd.read_csv(f'{GLOBAL_PATH}/{tower}_2017-2022_final-qc.csv', header=0, \n",
    "                     skipfooter=1, na_values=[-999, '-999'], engine='python', \n",
    "                     parse_dates=True)\n",
    "    \n",
    "        df['timestampUTC'] = pd.to_datetime(df['timestampUTC'], format='%Y%m%d%H%M%S').dt.tz_localize('UTC')\n",
    "        df = df.set_index('timestampUTC', drop=True)\n",
    "\n",
    "        tower_dfs_15m_clean.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c5356-e18a-4803-9785-bbf1e290706d",
   "metadata": {},
   "source": [
    "## Step 2: Detect extreme events per tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d7b8ae-6efd-466d-b01a-b4335dadda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event thresholds and required minimum duration (hours)\n",
    "EVENT_SPECS = {\n",
    "    # Event name: (required variables, condition function, minimum duration in hours)\n",
    "    \"E1_TempMoistHaz\": (\n",
    "        [\"TempC\", \"AbsHum\"],\n",
    "        lambda df, col: (df[col[\"TempC\"]] > 24.0) & (df[col[\"AbsHum\"]] > 20.0),\n",
    "        2.0\n",
    "    ),\n",
    "    \"E2_WindChill\": (\n",
    "        [\"TempC\", \"PkWSpdMph\"],\n",
    "        lambda df, col: (df[col[\"TempC\"]] <= 4.8) & (df[col[\"PkWSpdMph\"]] >= 3.0),\n",
    "        2.0\n",
    "    ),\n",
    "    \"E3_LowTemp_lt0\":   ([\"TempC\"], lambda df, col: df[col[\"TempC\"]] < 0.0,   2.0),\n",
    "    \"E3_LowTemp_lt-5\":  ([\"TempC\"], lambda df, col: df[col[\"TempC\"]] < -5.0,  2.0),\n",
    "    \"E3_LowTemp_lt-10\": ([\"TempC\"], lambda df, col: df[col[\"TempC\"]] < -10.0, 2.0),\n",
    "\n",
    "    \"E4_HighWind_Peak_gt25\": ([\"PkWSpdMph\"], lambda df, col: df[col[\"PkWSpdMph\"]] > 25.0, 1.0),\n",
    "\n",
    "    \"E5_LowWind_lt2\":   ([\"PkWSpdMph\"], lambda df, col: df[col[\"PkWSpdMph\"]] < 2.0,  3.0),\n",
    "    \"E5_LowWind_lt1\":   ([\"PkWSpdMph\"], lambda df, col: df[col[\"PkWSpdMph\"]] < 1.0,  3.0),\n",
    "    \"E5_LowWind_lt0_5\": ([\"PkWSpdMph\"], lambda df, col: df[col[\"PkWSpdMph\"]] < 0.5, 3.0),\n",
    "}\n",
    "\n",
    "# Sensor columns used for each tower \n",
    "colmap_per_tower = {\n",
    "    \"TOWA\": {\"TempC\":\"TempC_030m\",\"AbsHum\":\"AbsHum_015m\",\"PkWSpdMph\":\"PkWSpdMph_030m\"},\n",
    "    \"TOWB\": {\"TempC\":\"TempC_030m\",\"AbsHum\":None,          \"PkWSpdMph\":\"PkWSpdMph_030m\"},\n",
    "    \"TOWD\": {\"TempC\":\"TempC_035m\",\"AbsHum\":\"AbsHum_015m\",\"PkWSpdMph\":\"PkWSpdMph_035m\"},\n",
    "    \"TOWF\": {\"TempC\":\"TempC_010m\",\"AbsHum\":\"AbsHum_010m\",\"PkWSpdMph\":\"PkWSpdMph_010m\"},\n",
    "    \"TOWS\": {\"TempC\":\"TempC_025m\",\"AbsHum\":None,          \"PkWSpdMph\":\"PkWSpdMph_025m\"},\n",
    "    \"TOWY\": {\"TempC\":\"TempC_033m\",\"AbsHum\":None,          \"PkWSpdMph\":\"PkWSpdMph_033m\"},\n",
    "}\n",
    "\n",
    "# infer the median step size of the time index (minutes)\n",
    "def infer_step_minutes(index: pd.DatetimeIndex, fallback=15.0) -> float:\n",
    "    if len(index) < 2:\n",
    "        return float(fallback)\n",
    "    diffs = pd.Series(index).diff().dropna().dt.total_seconds() / 60.0\n",
    "    return float(diffs.median()) if not diffs.empty else float(fallback)\n",
    "\n",
    "# convert a boolean time series into continuous event segments\n",
    "def boolean_runs_to_segments(mask: pd.Series, min_duration_min: float) -> list:\n",
    "    \"\"\"\n",
    "    mask: Boolean time series (True = threshold condition is satisfied)\n",
    "    min_duration_min: required minimum segment length in minutes\n",
    "    Returns: list of (start, end, duration_minutes)\n",
    "    \"\"\"\n",
    "    if mask.empty:\n",
    "        return []\n",
    "    # Identify transition points (True blocks)\n",
    "    m = mask.astype(bool).copy()\n",
    "    change = m.ne(m.shift(1, fill_value=False))\n",
    "    starts = m & change\n",
    "    ends   = (~m) & change\n",
    "    start_times = list(mask.index[starts])\n",
    "    end_times   = list(mask.index[ends])\n",
    "\n",
    "    # If the series ends with True, close the last block at the end\n",
    "    if len(start_times) > len(end_times):\n",
    "        end_times.append(mask.index[-1])\n",
    "\n",
    "    segs = []\n",
    "    for st, et in zip(start_times, end_times):\n",
    "        dur = (et - st).total_seconds()/60.0\n",
    "        if dur >= min_duration_min:\n",
    "            segs.append((st, et, dur))\n",
    "    return segs\n",
    "\n",
    "#  Main function: classify events for one tower \n",
    "def classify_events_for_tower(df: pd.DataFrame, tower_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input: single tower dataframe (time index)\n",
    "    Output: DataFrame with event segments:\n",
    "        tower, event, start, end, duration_minutes\n",
    "    \"\"\"\n",
    "    cmap = colmap_per_tower.get(tower_name, {})\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    step_min = infer_step_minutes(idx, fallback=15.0)\n",
    "\n",
    "    out = []\n",
    "    for ev, (needed_keys, cond_fn, min_hours) in EVENT_SPECS.items():\n",
    "        # Skip events if the tower is missing required variables\n",
    "        ok = True\n",
    "        for k in needed_keys:\n",
    "            colname = cmap.get(k)\n",
    "            if (colname is None) or (colname not in df.columns):\n",
    "                ok = False\n",
    "                break\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        # Evaluate the condition; any row with NaN is treated as False\n",
    "        sub = df[[cmap[k] for k in needed_keys]].copy()\n",
    "        mask = cond_fn(df, cmap) & (~sub.isna().any(axis=1))\n",
    "        mask = pd.Series(mask.values, index=idx)\n",
    "\n",
    "        # Convert True blocks to event segments\n",
    "        segs = boolean_runs_to_segments(mask, min_duration_min=min_hours*60.0)\n",
    "        for st, et, dur in segs:\n",
    "            out.append({\n",
    "                \"tower\": tower_name,\n",
    "                \"event\": ev,\n",
    "                \"start\": st,\n",
    "                \"end\": et,\n",
    "                \"duration_minutes\": float(dur)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# Batch processing across all towers \n",
    "def classify_all_towers(tower_dfs_15m_clean: list, towers_of_interest: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input: list of tower DataFrames (same order as towers_of_interest)\n",
    "    Output: combined DataFrame with event segments across towers\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    for tower, df in zip(towers_of_interest, tower_dfs_15m_clean):\n",
    "        seg = classify_events_for_tower(df, tower)\n",
    "        all_rows.append(seg)\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(columns=[\"tower\",\"event\",\"start\",\"end\",\"duration_minutes\"])\n",
    "    out = pd.concat(all_rows, ignore_index=True).sort_values([\"tower\",\"event\",\"start\"])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0db3815-445b-4a2a-8e5f-33bf07141aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tower            event                     start                       end  \\\n",
      "0  TOWA  E1_TempMoistHaz 2017-07-08 00:30:00+00:00 2017-07-08 04:45:00+00:00   \n",
      "1  TOWA  E1_TempMoistHaz 2017-07-21 00:45:00+00:00 2017-07-21 05:30:00+00:00   \n",
      "2  TOWA  E1_TempMoistHaz 2017-07-22 00:00:00+00:00 2017-07-22 07:15:00+00:00   \n",
      "3  TOWA  E1_TempMoistHaz 2017-07-23 00:15:00+00:00 2017-07-23 05:30:00+00:00   \n",
      "4  TOWA  E1_TempMoistHaz 2017-07-23 05:45:00+00:00 2017-07-23 10:30:00+00:00   \n",
      "\n",
      "   duration_minutes  \n",
      "0             255.0  \n",
      "1             285.0  \n",
      "2             435.0  \n",
      "3             315.0  \n",
      "4             285.0  \n"
     ]
    }
   ],
   "source": [
    "segments = classify_all_towers(tower_dfs_15m_clean, towers_of_interest)\n",
    "segments.to_csv(\"event_segments_by_tower.csv\", index=False)\n",
    "print(segments.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf33858-f471-4557-8e83-2d5bff4ff555",
   "metadata": {},
   "source": [
    "## Step 3: Union events across towers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff59649-95ef-4bdb-9a7c-4b637f54e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_intervals(ivals, max_gap_minutes: float = 0.0):\n",
    "    \n",
    "    # Merge overlapping or adjacent intervals, also bridge small gaps (e.g., ≤ max_gap_minutes).\n",
    "    \n",
    "    gap = pd.Timedelta(minutes=float(max_gap_minutes))\n",
    "    merged = []\n",
    "    for st, et in ivals:\n",
    "        if not merged:\n",
    "            merged.append([st, et])\n",
    "            continue\n",
    "        lst_st, lst_et = merged[-1]\n",
    "        # If the new interval overlaps or nearly touches the last one, merge them\n",
    "        if st <= lst_et + gap:\n",
    "            if et > lst_et:\n",
    "                merged[-1][1] = et\n",
    "        else:\n",
    "            # Otherwise start a new block\n",
    "            merged.append([st, et])\n",
    "    return merged\n",
    "\n",
    "\n",
    "def union_segments_across_towers(\n",
    "    segments: pd.DataFrame,\n",
    "    max_gap_minutes: float = 0.0,\n",
    "    default_step_min: float = 15.0,\n",
    "    by_month: bool = True\n",
    ") -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Combine event detections across all towers into a single “union” timeline.\n",
    "    That means: if any tower detects the event, the union interval is counted.\n",
    "\n",
    "    \"\"\"\n",
    "    if segments.empty:\n",
    "        return (\n",
    "            pd.DataFrame(columns=[\"event\",\"start\",\"end\",\"duration_minutes\"]),\n",
    "            pd.DataFrame(columns=[\"year\",\"month\",\"event\",\"union_count\",\"union_total_minutes\"]) if by_month else None\n",
    "        )\n",
    "\n",
    "    seg = segments.copy()\n",
    "    seg[\"start\"] = pd.to_datetime(seg[\"start\"])\n",
    "    seg[\"end\"]   = pd.to_datetime(seg[\"end\"])\n",
    "    seg = seg.dropna(subset=[\"start\",\"end\"])\n",
    "\n",
    "    # merge intervals tower-by-tower into one union per event\n",
    "    out_rows = []\n",
    "    for ev, grp in seg.groupby(\"event\"):\n",
    "        # Collect all time intervals across towers for this event\n",
    "        ivals = grp[[\"start\",\"end\"]].sort_values(\"start\").to_numpy().tolist()\n",
    "        merged = _merge_intervals(ivals, max_gap_minutes=max_gap_minutes)\n",
    "\n",
    "        # Calculate duration of each merged interval\n",
    "        for st, et in merged:\n",
    "            dur_min = (et - st).total_seconds()/60.0\n",
    "            if dur_min <= 0:\n",
    "                dur_min = float(default_step_min)\n",
    "            out_rows.append({\n",
    "                \"event\": ev,\n",
    "                \"start\": st,\n",
    "                \"end\": et,\n",
    "                \"duration_minutes\": float(dur_min)\n",
    "            })\n",
    "\n",
    "    union_segments = (\n",
    "        pd.DataFrame(out_rows)\n",
    "        .sort_values([\"event\",\"start\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # monthly aggregation (count + total duration) \n",
    "    monthly_summary = None\n",
    "    if by_month and not union_segments.empty:\n",
    "        tmp = union_segments.copy()\n",
    "        tmp[\"year\"]  = tmp[\"start\"].dt.year\n",
    "        tmp[\"month\"] = tmp[\"start\"].dt.month\n",
    "        monthly_summary = (\n",
    "            tmp.groupby([\"year\",\"month\",\"event\"], as_index=False)\n",
    "               .agg(union_count=(\"duration_minutes\",\"size\"),\n",
    "                    union_total_minutes=(\"duration_minutes\",\"sum\"))\n",
    "               .sort_values([\"year\",\"month\",\"event\"])\n",
    "               .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    return union_segments, monthly_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a65faa3-bd11-4489-aaf4-efc5805268e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_seg, monthly_union = union_segments_across_towers(segments, max_gap_minutes=0, by_month=True)\n",
    "union_seg.to_csv(\"union_segments_per_event.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb3a18-b091-4f23-b77e-7b7a65b68d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
